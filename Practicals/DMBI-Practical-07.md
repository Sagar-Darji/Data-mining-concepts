# Practical 07: 
### Extract if-then rues from decision tree generated by classifier, Observe the confusion matrix and derive Accuracy, F-measure, TPrate, Fprate, Precision and recall values. Apply cross-validation strategy with various fold levels and compare the accuracy results. 
![](https://img.shields.io/badge/Name-Sagar_Darji-blue.svg?style=flat)
![](https://img.shields.io/badge/Enrollment.no-181310132010-blue.svg?style=flat)


Practical: 7
Decision Tree 

![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/Decision_Tree-2.png)

### If-Then rules: 
To extract a rule from a decision tree :
- One rule is created for each path from the root to the leaf node.
- To form a rule antecedent, each splitting criterion is logically ANDed.
- The leaf node holds the class prediction, forming the rule consequent.


`Example:-`
```
•	  If outlook= “Sunny”  And
    Humidity = “high”  Then 
	  Play = “No” 

•	  If outlook= “Overcast” Then
     Play= “Yes”

•	  If outlook = “ Rain” And
    Wind = “Strong” Then
	  Play = “No”
```

### Confusion Matrix:
A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. The number of correct and incorrect predictions are summarized with count values and broken down by each class.
 
![](https://editor.analyticsvidhya.com/uploads/70456have%20cancer.jpg)
```
•	TP (True positive): A true positive is an outcome where the model correctly predicts the positive class.

•	TN (True negative):  A true negative is an outcome where the model correctly predicts the negative class.

•	FP (False positive): A false positive is an outcome where the model incorrectly predicts the positive class.

•	FN (False negative): A false negative is an outcome where the model incorrectly predicts the negative class.
```

- ***Accuracy*** : 
```
It is the number of correct prediction out of total predictions.  

Accuracy =  (TP + TN) / Total 
```
- ***Recall / TP rate /Sensitivity :*** 
```
It is the total number of correctly predicted items for positive class out of total items of actual positive class .  

Recall = TP / Actual Positive
 =  TP / (TP + FN)
```
- ***Precision :*** 
```
It is the total number of correctly predicted items  for positive class out of total items of predicted positive class.

Precision = TP / Predicted Positive
	                = TP / (TP + FP)
```
- ***F-measures:*** 
```
It is the harmonic mean of precision and recall.

F1 score =  ( 2 * Precision * Recall )/ ( Precision + Recall )
```
- ***FP rate : *** 
```
It is the total number of falsely predicted items for positive class out of total items of actual negative class.

FP rate = FP / Actual Negative
            = FP / (FP + TN)
```

### Training and Testing:
- The data we use is usually split into training data and test data.
- The training set contains a known output, and the model learns on this data in order to be generalized to other data later on.
- We have the test dataset (or subset) in order to test our model’s prediction on this subset.
- What if one subset of our data has only people from a certain state, employees with a certain income level but not other income levels, only women or only people at a certain age?

![](https://www.learntek.org/blog/wp-content/uploads/2018/11/Ml-and-PR.jpg)
 
Cross Validation 
•	In order to avoid this, we can perform something called cross validation. 
•	It’s very similar to train/test split, but it’s applied to more subsets. Meaning, we split our data into k subsets, and train on k-1 one of those subset. 
•	What we do is to hold the last subset for test. We’re able to do it for each of the subsets. 
•	There are mainly 4 types of cross-validation used in machine learning to resample the dataset:- 
1.	Leave one out cross validation 
2.	K-fold cross validation 
3.	Stratified K-fold cross validation 
4.	Time series cross validation
### Leave one out cross validation:
- In this type of CV, we take/we leave out only one record out of all the available records and we perform testing on the record which is left out and we perform training on all the remaining tuples. 
- We can understand this better by observing the figure given below.
- This method is very computationally expensive and should be used on small datasets. If the dataset is big, it would most likely be better to use a different method, like kfold. 

![](https://miro.medium.com/max/2816/1*AVVhcmOs7WCBnpNhqi-L6g.png) 
 
### K fold Cross Validation:
- In K-Folds Cross Validation we split our data into k different subsets (or folds). 
- We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data.
- We then average the model against each of the folds and then finalize our model. After that we test it against the test set. 

![](https://miro.medium.com/max/601/1*PdwlCactbJf8F8C7sP-3gw.png)

### Stratified K-fold Cross Validation:
- The drawback of k-fold is solved with stratified version of it.
- This instance of CV is almost same as K-fold cross validation, just the difference that is found in this one is that every time the test data selected is having same no. of instances as that of the training dataset (drawback of K-fold) for each class.
- Thus, this Can be said as as improved version of K-fold CV. 

![](https://i.stack.imgur.com/B9CCp.png)
 
### Time Series Cross Validation:
- The time series CV is used when we have a dataset that has the values in terms of time series.
- Suppose we want to predict the stock on day 5 day 6 day 7 from day 1 day 2 day 3 day 4 data set this kind of cross validation is used. 
![](https://miro.medium.com/max/1362/1*WMJCAkveTgbdBveMMMZtUg.png)
 
 
